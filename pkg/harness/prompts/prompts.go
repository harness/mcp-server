package prompts

import (
	p "github.com/harness/harness-mcp/pkg/prompts"
	"github.com/mark3labs/mcp-go/server"
)

// RegisterPrompts initializes and registers predefined prompts with the MCP server.
func RegisterPrompts(mcpServer *server.MCPServer) {
	prompts := p.Prompts{}

	// This prompt is intended to make the LLM handle the date parameters in the correct format because fields descriptions where not enough.
	prompts.Append(
		p.NewPrompt().SetName("get_ccm_overview").
			SetDescription("Ensure parameters are provided correctly and in the right format. ").
			SetResultDescription("Input parameters validation").
			SetText(`{"standard": "When calling get_ccm_overview, ensure you have: accountIdentifier, groupBy, startDate, and endDate.\n\t\t\t\t\t- If any are missing, ask the user for the specific value(s).\n\t\t\t\t\t- Always send startDate and endDate in the following format: 'MM/DD/YYYY' (e.g. '10/30/2025')\n\t\t\t\t\t- If no dates are supplied, default startDate to 60 days ago and endDate to now."}`).
			Build())

	prompts.Append(
		p.NewPrompt().SetName("ask_confirmation_for_update_and_delete_operations").
			SetDescription("Ensure that Update or Delete operations are executed ONLY after user confirmation.").
			SetResultDescription("Execute operation if user input 'yes', cancel otherwise.").
			SetText(`{"standard": "**Confirmation Policy**:\nWhen a function/tool description contains the tag <INSERT_TOOL>, <UPDATE_TOOL> or <DELETE_TOOL>, **BEFORE** calling it you **ALWAYS** must:\n\n- Present a clear, minimal summary of the impending change (show key fields/values).\n- Ask: 'Please confirm to proceed (yes/no).'\n- **ONLY** invoke the tool if the user's next message is exactly \"yes\" (case-insensitive).\n- If the user's answer is anything other than \"yes\", do not call the tool; instead, offer to adjust or cancel.\n- Never assume consent; always re-ask if the context is ambiguous or stale."}`).
			Build())

	// Pipeline summarization prompt
	prompts.Append(
		p.NewPrompt().SetName("pipeline_summarizer").
			SetDescription("Summarize a Harness pipeline's structure, purpose, and behavior.").
			SetResultDescription("Comprehensive pipeline summary with key details.").
			SetText(`{
				"standard": "I need you to summarise the pipeline with the input pipeline identifier.\n    1. What to do?\n       - Fetch any required metadata or definitions for the pipeline.\n       - Analyze its configuration and structure.\n       - Make the necessary tool calls to get the pipeline related details.\n       - Before making any tool call, for all other tool calls you MUST send a message on what tool you are going to call and why.\n       - Produce a concise, accurate summary of the pipeline's design and behavior.\n    2. Tools to call to get a complete overview of pipeline:\n       - get_pipeline\n       - get_pipeline_summary\n       - list_pipelines\n       - get_environment\n       - get_service\n       - list_settings (with category as NOTIFICATIONS)\n       - get_secret\n       - list_triggers\n       - list_executions\n       - create_follow_up_prompt\n    3. Must-have details in the summary (acceptance criteria):\n       - Purpose and Objective: What this pipeline is designed to accomplish (e.g. 'Builds and deploys a Node.js microservice to staging and production.')\n       - High-Level Architecture: Major components and phases (build, test, security scanning, deployment).\n       - Environment Flow: How the execution moves through environments.\n       - Key Technologies: Languages, frameworks, deployment targets, and tools used.\n       - Trigger Conditions: What events start the pipeline (Git commits, manual triggers, schedules).\n       - Approval Gates: Any manual approvals required, and who must sign off.\n       - Dependencies: External dependencies such as environments, infrastructures, connectors, services, other pipelines this one relies on, etc with their ids if available.\n       - Success Criteria: What defines a successful run.\n    4. Instructions for calling create_follow_up_prompt tool:\n       Follow these steps in order:\n       a. Make a tool call to list_executions to fetch the execution ID of the latest execution of the given pipeline\n       b. Make a tool call to create_follow_up_prompt with these parameters:\n          - action_data: The following JSON data stringified:\n            {\"actions\": [\n              {\n                \"text\": \"View Latest Execution\",\n                \"action\": \"OPEN_ENTITY_NEW_TAB\",\n                \"data\": {\n                  \"pageName\": \"ExecutionPipelineView\",\n                  \"metadata\": {\n                     \"executionId\": \"executionId-value\",\n                     \"pipelineId\": \"pipeline-id\"\n                  }\n                }\n              },\n              {\n                \"text\": \"View Pipeline\",\n                \"action\": \"OPEN_ENTITY_NEW_TAB\",\n                \"data\": {\n                  \"pageName\": \"PipelineStudio\",\n                  \"metadata\": {\n                     \"id\": \"pipeline-id\"\n                  }\n                }\n              }\n            ]}\n          - Replace \"executionId-value\" with the actual execution ID from the list_executions response\n          - Replace \"pipeline-id\" with the pipeline identifier\n          - If there are no executions available, MUST use this JSON data instead(with only option view pipeline):\n            {\"actions\": [\n              {\n                \"text\": \"View Pipeline\",\n                \"action\": \"OPEN_ENTITY_NEW_TAB\",\n                \"data\": {\n                  \"pageName\": \"PIPELINE_STUDIO\",\n                  \"metadata\": {\n                     \"id\": \"pipeline-id\"\n                  }\n                }\n              }\n            ]}\n\n    5. Output format:\n       Return the following summary data ONLY in a markdown format. DO NOT add any other TEXT or Headings. DO NOT use JSON literals:\n       {\n         \"### Purpose\":       string,\n         \"### Architecture\":  string,\n         \"### Environment\":   string,\n         \"### Technologies\":  string[],\n         \"### Triggers\":      string[],\n         \"### Approvals\":     string[],\n         \"### Dependencies\":  string[],\n         \"### Success Criteria\": string,\n         \"### Past Execution Details\": string[]\n       } \n       Return the tool response exactly as received, in markdown format. DO NOT use JSON literals. The pipeline summary should the final response."
			  }`).
			Build())

	prompts.Append(
		p.NewPrompt().SetName("ask_release_agent_prompt").
			SetDescription("Prompt for the Ask Release Agent tool").
			SetResultDescription("Ask Release Agent prompt").
			SetText(`{"standard": "When calling ask_release_agent, ensure you have the required parameters for release process operations:\n- First, clarify the user's release requirements; echo back your understanding before calling the tool.\n- Include relevant context for release processes, such as:\n  - Release process requirements\n- The RMG AI DevOps Agent specializes in creating release processes and will automatically use the CREATE_PROCESS action.\n- Provide detailed requirements to get the most accurate release process configuration.\n- Do not use the words \"pipeline\" or \"stages\" in your response."}`).
			Build())

	// Pipeline error analysis prompt
	prompts.Append(
		p.NewPrompt().SetName("pipeline_error_analysis").
			SetDescription("Comprehensive error analysis for failed pipelines").
			SetResultDescription("Structured error analysis report").
			SetText(`{
	  "standard": "Perform a comprehensive error analysis for the failed pipeline step and provide a structured, detailed report. Make sure there are no typos.\n\n1. Input Parameters:\n   - log_key: The log key of the failed step node (required for downloading step-specific logs)\n   - pipeline_id: Pipeline identifier\n   - pipeline_execution_id: pipeline execution identifier\n   - stage_node_id: Stage node identifier\n   - child_stage_node_id: Optional child stage node identifier\n   - step_id: Step identifier of the failed step\n\n2. Analysis Process:\n   a. Initial Log Analysis:\n      - Use download_execution_logs with the provided log_key to fetch logs (default: last 50 lines, max: 100 lines).\n      - If root cause is not clear:\n        * Use get_pipeline to understand pipeline structure, stage and step identifiers.\n          - Carefully analyze the pipeline structure to identify the current stage.\n          - Within the current stage, identify all steps that execute before the failed step.\n          - Map out the execution sequence and dependencies between steps.\n        * Use get_execution with stage_node_id and child_stage_node_id (if present) to get pipeline execution data and log base keys for all steps in the relevant stage.\n        * Analyze logs from up to 3 previous steps in the same stage:\n          - Start with the immediately preceding step in the execution sequence.\n          - Only fetch logs from earlier steps if necessary based on dependencies.\n          - Use download_execution_logs with each step's log base key.\n          - Pay special attention to steps that might affect the failed step's inputs or environment.\n\n   b. Change Impact Analysis:\n      - Use list_executions to find the timestamp of the last successful run.\n      - Use list_user_audits with:\n        * start_time: last success timestamp (or 1 week ago)\n        * end_time: pipeline execution timestamp (from execution data)\n        * resource_type: PIPELINE\n        * actions: UPDATE, CREATE\n        * resource_identifier: pipeline_id\n      - Convert timestamps to ISO 8601.\n      - Extract audit IDs and call get_audit_yaml for each to analyze configuration changes.\n\n   c. Additional Analysis:\n      - Historical Pattern Matching: Compare with previous similar failures to identify recurring issues.\n      - Failure Timeline Reconstruction: Build a sequence of events leading to the failure.\n      - Contextual Recommendations: Provide prioritized remediation actions based on severity and environment.\n      - Preventive Measures: Suggest improvements to prevent recurrence.\n      - Impact Assessment: Identify dependent resources affected.\n\n   d. Error Categorization:\n      When determining the error type, use one of the following categories:\n      - build_failure: Issues with compilation, building artifacts, or code generation\n      - test_failure: Test cases failing during execution\n      - deployment_issue: Problems deploying to target environments\n      - infrastructure_problem: Issues with underlying infrastructure or resources\n      - configuration_error: Misconfigurations in pipeline, environment, or service definitions\n      - external_dependency_failure: Failures in external services, APIs, or dependencies\n\n3. Tools to Call (in sequence):\n   - download_execution_logs (primary and previous steps)\n   - get_pipeline (if needed)\n   - get_execution\n   - list_executions\n   - list_user_audits\n   - get_audit_yaml\n   - Additional as needed:\n     * list_pipeline_triggers\n     * get_environment\n     * get_service\n     * get_connector_details\n     * get_secret\n     * get_repository\n   - create_follow_up_prompt\n\n4. Instructions for calling create_follow_up_prompt tool:\n   a. Make a tool call to list_executions to fetch the execution ID of the latest execution of the given pipeline.\n   b. Make a tool call to create_follow_up_prompt with these parameters:\n      - action_data: The following JSON data stringified:\n        {\"actions\": [\n          {\n            \"text\": \"Show Me the Audit trail\",\n            \"action\": \"OPEN_ENTITY_NEW_TAB\",\n            \"data\": {\n              \"pageName\": \"AuditTrailsPage\",\n              \"metadata\": {\n                 \"resourceType\": \"PIPELINE\",\n                 \"actions\": \"UPDATE,CREATE\",\n                 \"pipeline-id\": \"pipeline-id\"\n              }\n            }\n          }\n        ]}\n      - Replace \"pipeline-id\" with the pipeline identifier.\n   c. Make another tool call to create_follow_up_prompt with:\n      - action_data: \"Help me fix the pipeline yaml\"\n\n5. Output Format:\n   Return ONLY the following report in markdown format. DO NOT add any other TEXT or Headings. DO NOT use JSON literals.  Follow these formatting guidelines strictly:\n   - Ensure ALL markdown headings have NO extra spaces before the '#' symbols\n   - Maintain consistent indentation throughout the document\n   - Use proper markdown hierarchy (# for main title, ## for sections, ### for subsections)\n   - Do not include any extra text or commentary outside the report template  \n\n## Error Categorization\n   **Error Type**: <error_type>\n\n   ## Root Cause Analysis\n   ### Summary\n   <summary_text>\n   ### Evidence\n   <list_of_evidence>\n\n   ## Change Impact Correlation\n   ### Recent Changes\n   - **Timestamp**: <timestamp>\n   - **Author**: <author>\n   - **Description**: <description>\n   ### External Dependencies\n   - **Type**: <dependency_type>\n   - **Name**: <name>\n   - **Status**: <status>\n\n   ## Historical Pattern Analysis\n   ### Similar Failures\n   - **Previous Failure ID**: <failure_id>\n   - **Similarity Score**: <score>\n   - **Matched Pattern**: <pattern>\n   - **Resolution Applied**: <resolution>\n\n   ## Recommendations\n   ### Action Items\n   - **Action**: <action>\n     - **Priority**: <priority>\n     - **Justification**: <justification>\n\n   ## Impact Assessment\n   ### Affected Services\n   <list_of_services>\n   ### Risk Level\n   <risk_level>\n\n   ---\n   **Generated at:** <timestamp>"
	}`).
			Build())

	p.AddPrompts(prompts, mcpServer)
}
