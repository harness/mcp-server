package prompts

import (
	p "github.com/harness/harness-mcp/pkg/prompts"
	"github.com/mark3labs/mcp-go/server"
)

// RegisterPrompts initializes and registers predefined prompts with the MCP server.
func RegisterPrompts(mcpServer *server.MCPServer) {
	prompts := p.Prompts{}

	// This prompt is intended to make the LLM handle the date parameters in the correct format because fields descriptions where not enough.
	prompts.Append(
		p.NewPrompt().SetName("get_ccm_overview").
			SetDescription("Ensure parameters are provided correctly and in the right format. ").
			SetResultDescription("Input parameters validation").
			SetText(`{"standard": "When calling get_ccm_overview, ensure you have: accountIdentifier, groupBy, startDate, and endDate.\n\t\t\t\t\t- If any are missing, ask the user for the specific value(s).\n\t\t\t\t\t- Always send startDate and endDate in the following format: 'MM/DD/YYYY' (e.g. '10/30/2025')\n\t\t\t\t\t- If no dates are supplied, default startDate to 60 days ago and endDate to now."}`).
			Build())

	prompts.Append(
		p.NewPrompt().SetName("ask_confirmation_for_update_and_delete_operations").
			SetDescription("Ensure that Update or Delete operations are executed ONLY after user confirmation.").
			SetResultDescription("Execute operation if user input 'yes', cancel otherwise.").
			SetText(`{"standard": "**Confirmation Policy**:\nWhen a function/tool description contains the tag <INSERT_TOOL>, <UPDATE_TOOL> or <DELETE_TOOL>, **BEFORE** calling it you **ALWAYS** must:\n\n- Present a clear, minimal summary of the impending change (show key fields/values).\n- Ask: 'Please confirm to proceed (yes/no).'\n- **ONLY** invoke the tool if the user's next message is exactly \"yes\" (case-insensitive).\n- If the user's answer is anything other than \"yes\", do not call the tool; instead, offer to adjust or cancel.\n- Never assume consent; always re-ask if the context is ambiguous or stale."}`).
			Build())

	// Pipeline summarization prompt
	prompts.Append(
		p.NewPrompt().SetName("pipeline_summarizer").
			SetDescription("Summarize a Harness pipeline's structure, purpose, and behavior.").
			SetResultDescription("Comprehensive pipeline summary with key details.").
			SetText(`{
				"standard": "I need you to summarise the pipeline with the input pipeline identifier.\n    1. What to do?\n       - Fetch any required metadata or definitions for the pipeline.\n       - Analyze its configuration and structure.\n       - Make the necessary tool calls to get the pipeline related details.\n       - Before making any tool call, for all other tool calls you MUST send a message on what tool you are going to call and why.\n       - Produce a concise, accurate summary of the pipeline's design and behavior.\n    2. Tools to call to get a complete overview of pipeline:\n       - get_pipeline\n       - get_pipeline_summary\n       - list_pipelines\n       - get_environment\n       - get_service\n       - list_settings (with category as NOTIFICATIONS)\n       - get_secret\n       - list_triggers\n       - list_executions\n       - create_follow_up_prompt\n    3. Must-have details in the summary (acceptance criteria):\n       - Purpose and Objective: What this pipeline is designed to accomplish (e.g. 'Builds and deploys a Node.js microservice to staging and production.')\n       - High-Level Architecture: Major components and phases (build, test, security scanning, deployment).\n       - Environment Flow: How the execution moves through environments.\n       - Key Technologies: Languages, frameworks, deployment targets, and tools used.\n       - Trigger Conditions: What events start the pipeline (Git commits, manual triggers, schedules).\n       - Approval Gates: Any manual approvals required, and who must sign off.\n       - Dependencies: External dependencies such as environments, infrastructures, connectors, services, other pipelines this one relies on, etc with their ids if available.\n       - Success Criteria: What defines a successful run.\n    4. Instructions for calling create_follow_up_prompt tool:\n       Follow these steps in order:\n       a. Make a tool call to list_executions to fetch the execution ID of the latest execution of the given pipeline\n       b. Make a tool call to create_follow_up_prompt with these parameters:\n          - action_data: The following JSON data stringified:\n            {\"actions\": [\n              {\n                \"text\": \"View Latest Execution\",\n                \"action\": \"OPEN_ENTITY_NEW_TAB\",\n                \"data\": {\n                  \"pageName\": \"ExecutionPipelineView\",\n                  \"metadata\": {\n                     \"executionId\": \"executionId-value\",\n                     \"pipelineId\": \"pipeline-id\"\n                  }\n                }\n              },\n              {\n                \"text\": \"View Pipeline\",\n                \"action\": \"OPEN_ENTITY_NEW_TAB\",\n                \"data\": {\n                  \"pageName\": \"PipelineStudio\",\n                  \"metadata\": {\n                     \"id\": \"pipeline-id\"\n                  }\n                }\n              }\n            ]}\n          - Replace \"executionId-value\" with the actual execution ID from the list_executions response\n          - Replace \"pipeline-id\" with the pipeline identifier\n          - If there are no executions available, MUST use this JSON data instead(with only option view pipeline):\n            {\"actions\": [\n              {\n                \"text\": \"View Pipeline\",\n                \"action\": \"OPEN_ENTITY_NEW_TAB\",\n                \"data\": {\n                  \"pageName\": \"PIPELINE_STUDIO\",\n                  \"metadata\": {\n                     \"id\": \"pipeline-id\"\n                  }\n                }\n              }\n            ]}\n\n    5. Output format:\n       Return the following summary data ONLY in a markdown format, DO NOT use JSON literals:\n       {\n         \"### Purpose\":       string,\n         \"### Architecture\":  string,\n         \"### Environment\":   string,\n         \"### Technologies\":  string[],\n         \"### Triggers\":      string[],\n         \"### Approvals\":     string[],\n         \"### Dependencies\":  string[],\n         \"### Success Criteria\": string,\n         \"### Past Execution Details\": string[]\n       } \n       Return the tool response exactly as received, in markdown format. DO NOT use JSON literals. The pipeline summary should the final response."
			  }`).
			Build())

	prompts.Append(
		p.NewPrompt().SetName("ask_release_agent_prompt").
			SetDescription("Prompt for the Ask Release Agent tool").
			SetResultDescription("Ask Release Agent prompt").
			SetText(`{"standard": "When calling ask_release_agent, ensure you have the required parameters for release process operations:\n- First, clarify the user's release requirements; echo back your understanding before calling the tool.\n- Include relevant context for release processes, such as:\n  - Release process requirements\n- The RMG AI DevOps Agent specializes in creating release processes and will automatically use the CREATE_PROCESS action.\n- Provide detailed requirements to get the most accurate release process configuration.\n- Do not use the words \"pipeline\" or \"stages\" in your response."}`).
			Build())

	// Pipeline error analysis prompt
	prompts.Append(
		p.NewPrompt().SetName("pipeline_error_analysis").
			SetDescription("Comprehensive error analysis for failed pipelines").
			SetResultDescription("Structured error analysis report").
			SetText(`{"standard": "Perform a comprehensive error analysis for the failed pipeline run with the execution ID and provide a structured, detailed report.\n\nWhat to do?\n\n1. First, download the execution logs using the download_execution_logs tool to get detailed information about the pipeline execution.\n\n2. Retrieve execution data and any related metadata for the given pipeline execution ID.\n\nIntelligent Error Categorisation: Classify each error by type for every failed step and stage (e.g., build failure, test failure, deployment issue, infrastructure problem, configuration error, external dependency failure).\n\nRoot Cause Identification: Analyze logs and execution context to pinpoint the exact cause of the failure.\n\nChange Impact Correlation: Identify recent pipeline changes, code commits, and configuration updates that may have introduced the error. Follow these steps:\n\n1. Use the list_executions tool to find the timestamp of the pipeline's last successful execution.\n2. Convert this timestamp to ISO 8601 format (YYYY-MM-DDThh:mm:ss.sssZ) and use it as the start_time parameter for list_user_audits.\n3. Use the current time in ISO 8601 format as the end_time parameter.\n4. If the pipeline has never succeeded or run before, pass the timestamp for last week as start_time and current time as end_time parameters to list_user_audits.\n5. Call list_user_audits with resource_type=PIPELINE and actions=UPDATE,CREATE to find pipeline yaml changes.\n6. For each audit ID returned in the response, make a separate call to get_audit_yaml to retrieve the YAML diff of changes.\n7. Carefully analyze these YAML diffs to identify configuration changes that could have caused the failure.\n\nEnvironmental Context Analysis: Identify any changes or updates to the infrastructure definitions used by the pipeline at the time of failure, based on available audit records. \n\nHistorical Pattern Matching: Compare with previous similar failures to identify recurring issues and known solutions.\n\nFailure Timeline Reconstruction: Produce a timeline of what happened in the pipeline before it failed.\n\nContextual Recommendations: Provide prioritized remediation actions based on error type, severity, and environmental context.\n\nPreventive Measures: Suggest improvements to prevent similar failures in the future.\n\nImpact Assessment: Determine the dependent resources that may be affected by this failure.\n\nTools to call in sequence(Don't call any other tools unless specifically requested):\n\n1. download_execution_logs - FIRST, to get detailed logs for analysis\n2. get_execution - to get execution details\n3. get_pipeline - to understand pipeline structure\n4. list_executions - to find when the pipeline last passed successfully\n5. list_user_audits - with resource filter as PIPELINE and actions=UPDATE,CREATE\n6. get_audit_yaml - for each audit ID to analyze configuration changes\n7. Additional tools as needed:\n   - list_pipeline_triggers\n   - get_environment\n   - get_service\n   - get_connector_details\n   - get_secret\n   - get_repository\n\nMust-have details in the output:\n- Pipeline Execution Context:\n- pipeline_id, execution_id\n\nError Categorisation:\n- Per step and stage classification (error type)\n\nRoot Cause Analysis:\n- Specific cause\n- Supporting evidence (log excerpts, config diffs)\n\nChange Impact Correlation:\n- Relevant updates with timestamps, authors, and description\n\nEnvironmental Context:\n- Infrastructure status, resource usage, external dependency health at failure time, etc\n\nHistorical Pattern Matching:\n- Similar past failures, matched patterns, and solutions applied\n\nFailure Timeline:\n- Ordered events from pipeline start to failure point\n\nRemediation:\n- Action items with priority/severity\n\nPreventive Measures:\n- Suggested long-term fixes\n\nImpact Assessment:\n- Affected downstream resources and risk level\n\nOutput format:\nReturn ONLY the following data for each failed step ONLY in markdown format not in JSON format, do not add any extra information:\n\n\n\n{\n  \"pipeline_id\": \"string\",\n  \"execution_id\": \"string\",\n  \"run_sequence\": \"string\",\n  \"error_categorisation\": [\n    {\n      \"stage\": \"string\",\n      \"step\": \"string\",\n      \"error_type\": \"build_failure | test_failure | deployment_issue | infrastructure_problem | configuration_error | external_dependency_failure\"\n    }\n  ],\n  \"root_cause_analysis\": {\n    \"summary\": \"string\",\n    \"evidence\": [\"string\"]\n  },\n  \"change_impact_correlation\": [\n    {\n      \"timestamp\": \"string\",\n      \"author\": \"string\",\n      \"description\": \"string\"\n    }\n  ],\n  \"environmental_context\": {\n    \"infrastructure_status\": \"string\",\n    \"external_dependency_status\": [\n      { \"dependency_type\": \"string\", \"name\": \"string\", \"status\": \"string\" }\n    ]\n  },\n  \"historical_pattern_matching\": [\n    {\n      \"previous_failure_id\": \"string\",\n      \"similarity_score\": \"string\",\n      \"matched_pattern\": \"string\",\n      \"resolution_applied\": \"string\"\n    }\n  ],\n  \"failure_timeline\": [\n    {\n      \"timestamp\": \"string\",\n      \"event\": \"string\",\n      \"details\": \"string\"\n    }\n  ],\n  \"recommendations\": [\n    {\n      \"action\": \"string\",\n      \"priority\": \"string\",\n      \"justification\": \"string\"\n    }\n  ],\n  \"preventive_measures\": [\"string\"],\n  \"impact_assessment\": {\n    \"affected_services\": [\"string\"],\n    \"risk_level\": \"string\"\n  },\n  \"generated_at\": \"string\"\n}"}`).
			Build())

	p.AddPrompts(prompts, mcpServer)
}
