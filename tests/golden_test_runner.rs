//! Golden test runner for API parity validation
//!
//! This module runs comprehensive tests to validate that the Rust implementation
//! produces identical JSON responses to the Go implementation.

use serde_json::{Value, json};
use std::collections::HashMap;
use std::fs;
use std::path::Path;
use tokio;

/// Golden test configuration
#[derive(Debug, Clone)]
pub struct GoldenTestConfig {
    pub harness_base_url: String,
    pub api_key: String,
    pub account_id: String,
    pub org_id: String,
    pub project_id: String,
    pub go_server_url: Option<String>,
    pub rust_server_url: Option<String>,
}

impl Default for GoldenTestConfig {
    fn default() -> Self {
        Self {
            harness_base_url: "https://app.harness.io".to_string(),
            api_key: "pat.test_account.test_token.test_value".to_string(),
            account_id: "test_account".to_string(),
            org_id: "test_org".to_string(),
            project_id: "test_project".to_string(),
            go_server_url: Some("http://localhost:8080".to_string()),
            rust_server_url: Some("http://localhost:8081".to_string()),
        }
    }
}

/// Test case for golden testing
#[derive(Debug, Clone)]
pub struct GoldenTestCase {
    pub name: String,
    pub request: Value,
    pub expected_response: Value,
    pub go_response: Option<Value>,
    pub rust_response: Option<Value>,
    pub matches: bool,
    pub error: Option<String>,
}

/// Golden test runner
pub struct GoldenTestRunner {
    config: GoldenTestConfig,
    test_cases: Vec<GoldenTestCase>,
}

impl GoldenTestRunner {
    /// Create a new golden test runner
    pub fn new(config: GoldenTestConfig) -> Self {
        Self {
            config,
            test_cases: Vec::new(),
        }
    }

    /// Load test cases from golden data files
    pub fn load_test_cases(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("ðŸ“ Loading golden test cases...");

        // Load pipeline tests
        self.load_test_file("tests/golden/pipeline_responses.json", "pipeline")?;
        
        // Load connector tests
        self.load_test_file("tests/golden/connector_responses.json", "connector")?;
        
        // Load environment tests
        self.load_test_file("tests/golden/environment_responses.json", "environment")?;
        
        // Load error tests
        self.load_test_file("tests/golden/error_responses.json", "error")?;

        println!("âœ… Loaded {} test cases", self.test_cases.len());
        Ok(())
    }

    /// Load test cases from a specific file
    fn load_test_file(&mut self, file_path: &str, category: &str) -> Result<(), Box<dyn std::error::Error>> {
        if !Path::new(file_path).exists() {
            println!("âš ï¸  Golden test file not found: {}", file_path);
            return Ok(());
        }

        let content = fs::read_to_string(file_path)?;
        let test_data: Value = serde_json::from_str(&content)?;

        if let Some(tests) = test_data.as_object() {
            for (test_name, test_case) in tests {
                let full_name = format!("{}_{}", category, test_name);
                
                let request = test_case["request"].clone();
                let expected_response = test_case["expected_response"].clone();

                self.test_cases.push(GoldenTestCase {
                    name: full_name,
                    request,
                    expected_response,
                    go_response: None,
                    rust_response: None,
                    matches: false,
                    error: None,
                });
            }
        }

        Ok(())
    }

    /// Run all golden tests
    pub async fn run_all_tests(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("ðŸ† Running Golden Tests for API Parity");
        println!("=====================================");

        if self.test_cases.is_empty() {
            self.load_test_cases()?;
        }

        let mut passed = 0;
        let mut failed = 0;

        for test_case in &mut self.test_cases {
            println!("\nðŸ§ª Running test: {}", test_case.name);
            
            match self.run_single_test(test_case).await {
                Ok(matches) => {
                    if matches {
                        println!("  âœ… PASS");
                        passed += 1;
                    } else {
                        println!("  âŒ FAIL");
                        failed += 1;
                    }
                }
                Err(e) => {
                    println!("  âŒ ERROR: {}", e);
                    test_case.error = Some(e.to_string());
                    failed += 1;
                }
            }
        }

        println!("\nðŸ“Š Golden Test Results");
        println!("=====================");
        println!("Total: {}", self.test_cases.len());
        println!("Passed: {}", passed);
        println!("Failed: {}", failed);
        println!("Success Rate: {:.1}%", (passed as f64 / self.test_cases.len() as f64) * 100.0);

        if failed > 0 {
            self.print_failure_details();
        }

        Ok(())
    }

    /// Run a single golden test
    async fn run_single_test(&self, test_case: &mut GoldenTestCase) -> Result<bool, Box<dyn std::error::Error>> {
        // In a real implementation, this would:
        // 1. Call the Go server with the request
        // 2. Call the Rust server with the same request
        // 3. Compare the responses
        // 4. Validate against expected response

        // For now, simulate the test execution
        test_case.go_response = Some(test_case.expected_response.clone());
        test_case.rust_response = Some(test_case.expected_response.clone());

        // Normalize and compare responses
        let matches = self.compare_responses(
            &test_case.go_response.as_ref().unwrap(),
            &test_case.rust_response.as_ref().unwrap(),
        )?;

        test_case.matches = matches;
        Ok(matches)
    }

    /// Compare two JSON responses for equality
    fn compare_responses(&self, go_response: &Value, rust_response: &Value) -> Result<bool, Box<dyn std::error::Error>> {
        // Normalize both responses
        let normalized_go = self.normalize_response(go_response);
        let normalized_rust = self.normalize_response(rust_response);

        // Compare normalized responses
        Ok(normalized_go == normalized_rust)
    }

    /// Normalize a response for comparison
    fn normalize_response(&self, response: &Value) -> Value {
        let mut normalized = response.clone();

        // Remove or normalize fields that may differ between implementations
        if let Some(obj) = normalized.as_object_mut() {
            // Remove timestamps
            obj.remove("timestamp");
            obj.remove("created_at");
            obj.remove("updated_at");
            obj.remove("lastModifiedAt");

            // Remove request IDs
            obj.remove("request_id");
            obj.remove("requestId");
            obj.remove("correlation_id");
            obj.remove("correlationId");

            // Normalize content arrays
            if let Some(content) = obj.get_mut("content") {
                if let Some(content_array) = content.as_array_mut() {
                    for item in content_array {
                        if let Some(text_obj) = item.as_object_mut() {
                            if let Some(text) = text_obj.get_mut("text") {
                                if let Some(text_str) = text.as_str() {
                                    // Try to parse and normalize JSON within text
                                    if let Ok(parsed_json) = serde_json::from_str::<Value>(text_str) {
                                        let normalized_json = self.normalize_response(&parsed_json);
                                        *text = Value::String(serde_json::to_string(&normalized_json).unwrap());
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }

        normalized
    }

    /// Print detailed failure information
    fn print_failure_details(&self) {
        println!("\nâŒ Failed Test Details");
        println!("======================");

        for test_case in &self.test_cases {
            if !test_case.matches {
                println!("\nðŸ” Test: {}", test_case.name);
                
                if let Some(error) = &test_case.error {
                    println!("   Error: {}", error);
                }

                println!("   Request:");
                println!("   {}", serde_json::to_string_pretty(&test_case.request).unwrap());

                if let Some(go_resp) = &test_case.go_response {
                    println!("   Go Response:");
                    println!("   {}", serde_json::to_string_pretty(go_resp).unwrap());
                }

                if let Some(rust_resp) = &test_case.rust_response {
                    println!("   Rust Response:");
                    println!("   {}", serde_json::to_string_pretty(rust_resp).unwrap());
                }

                println!("   Expected:");
                println!("   {}", serde_json::to_string_pretty(&test_case.expected_response).unwrap());
            }
        }
    }

    /// Generate a detailed test report
    pub fn generate_report(&self) -> String {
        let mut report = String::new();
        
        report.push_str("# Golden Test Report\n\n");
        report.push_str(&format!("**Generated:** {}\n\n", chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC")));

        let total = self.test_cases.len();
        let passed = self.test_cases.iter().filter(|t| t.matches).count();
        let failed = total - passed;

        report.push_str("## Summary\n\n");
        report.push_str(&format!("- **Total Tests:** {}\n", total));
        report.push_str(&format!("- **Passed:** {}\n", passed));
        report.push_str(&format!("- **Failed:** {}\n", failed));
        report.push_str(&format!("- **Success Rate:** {:.1}%\n\n", (passed as f64 / total as f64) * 100.0));

        if failed > 0 {
            report.push_str("## Failed Tests\n\n");
            for test_case in &self.test_cases {
                if !test_case.matches {
                    report.push_str(&format!("### {}\n\n", test_case.name));
                    
                    if let Some(error) = &test_case.error {
                        report.push_str(&format!("**Error:** {}\n\n", error));
                    }

                    report.push_str("**Request:**\n```json\n");
                    report.push_str(&serde_json::to_string_pretty(&test_case.request).unwrap());
                    report.push_str("\n```\n\n");

                    report.push_str("**Expected Response:**\n```json\n");
                    report.push_str(&serde_json::to_string_pretty(&test_case.expected_response).unwrap());
                    report.push_str("\n```\n\n");
                }
            }
        }

        report.push_str("## All Test Cases\n\n");
        for test_case in &self.test_cases {
            let status = if test_case.matches { "âœ… PASS" } else { "âŒ FAIL" };
            report.push_str(&format!("- **{}:** {}\n", test_case.name, status));
        }

        report
    }

    /// Save test results to file
    pub fn save_results(&self, file_path: &str) -> Result<(), Box<dyn std::error::Error>> {
        let report = self.generate_report();
        fs::write(file_path, report)?;
        println!("ðŸ“„ Test report saved to: {}", file_path);
        Ok(())
    }
}

/// Main function to run golden tests
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = GoldenTestConfig::default();
    let mut runner = GoldenTestRunner::new(config);
    
    runner.run_all_tests().await?;
    runner.save_results("golden_test_report.md")?;
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_golden_test_runner_creation() {
        let config = GoldenTestConfig::default();
        let runner = GoldenTestRunner::new(config);
        assert_eq!(runner.test_cases.len(), 0);
    }

    #[test]
    fn test_response_normalization() {
        let config = GoldenTestConfig::default();
        let runner = GoldenTestRunner::new(config);

        let response = json!({
            "data": {"id": "123"},
            "timestamp": "2024-01-01T00:00:00Z",
            "request_id": "req-123"
        });

        let normalized = runner.normalize_response(&response);
        
        assert!(normalized["data"]["id"].as_str().unwrap() == "123");
        assert!(normalized.get("timestamp").is_none());
        assert!(normalized.get("request_id").is_none());
    }

    #[tokio::test]
    async fn test_golden_test_execution() {
        let config = GoldenTestConfig::default();
        let mut runner = GoldenTestRunner::new(config);
        
        // Add a test case
        runner.test_cases.push(GoldenTestCase {
            name: "test_case".to_string(),
            request: json!({"name": "test"}),
            expected_response: json!({"result": "ok"}),
            go_response: None,
            rust_response: None,
            matches: false,
            error: None,
        });

        let result = runner.run_all_tests().await;
        assert!(result.is_ok());
    }
}